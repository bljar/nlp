{"cells":[{"cell_type":"markdown","metadata":{"id":"frDPBrPvqOWQ"},"source":["# Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook"]},{"cell_type":"markdown","metadata":{"id":"jSb9BpFaqOWT"},"source":["Normally you would import `numpy` and rename it as `np`. \n","\n","However in this week's assignment you will notice that this convention has been changed. \n","\n","Now standard `numpy` is not renamed and `trax.fastmath.numpy` is renamed as `np`. \n","\n","The rationale behind this change is that you will be using Trax's numpy (which is compatible with JAX) far more often. Trax's numpy supports most of the same functions as the regular numpy so the change won't be noticeable in most cases.\n"]},{"cell_type":"code","source":["# work on gdrive with colab\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/MyDrive/NLP/GitHub/C3_Sequence_Models/02_Recurrent_Neural_Networks_for_Language_Modelling"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lqapqfyrtcwh","executionInfo":{"status":"ok","timestamp":1678169812221,"user_tz":-420,"elapsed":33589,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"0d4aada4-d684-48da-fd9d-25ce03362a6c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/MyDrive/NLP/GitHub/C3_Sequence_Models/02_Recurrent_Neural_Networks_for_Language_Modelling\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXJ5_gb5qOWU","executionInfo":{"status":"ok","timestamp":1678169230326,"user_tz":-420,"elapsed":65295,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"fc37334a-5528-4929-c1e1-f632ab238705"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.9/637.9 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -U -q trax\n","\n","import numpy\n","import trax\n","import trax.fastmath.numpy as np\n","\n","# Setting random seeds\n","numpy.random.seed(32)"]},{"cell_type":"markdown","metadata":{"id":"40PBsbaZqOWW"},"source":["One important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get `numpy.ndarray` but with Trax's numpy you will get `jax.interpreters.xla.DeviceArray`. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don't worry about it, treat it like you would treat an ndarray and march ahead.\n","\n","You can get a randomized numpy array by using the `numpy.random.random()` function.\n","\n","This is one of the functionalities that Trax's numpy does not currently support in the same way as the regular numpy. "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd-QhI7LqOWX","executionInfo":{"status":"ok","timestamp":1678169627749,"user_tz":-420,"elapsed":677,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"1227385d-4ec8-4040-f867-dd2690f2a915"},"outputs":[{"output_type":"stream","name":"stdout","text":["The regular numpy array looks like this:\n","\n"," [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n","  0.10108656 0.92848807 0.60910917 0.59655344]\n"," [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n","  0.58940123 0.04993276 0.56179184 0.76635847]\n"," [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n","  0.16242374 0.70937058 0.16062408 0.81077677]\n"," [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n","  0.67483453 0.78238275 0.69520163 0.32895445]\n"," [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n","  0.39807245 0.72790532 0.86333097 0.02616954]]\n","\n","It is of type: <class 'numpy.ndarray'>\n"]}],"source":["numpy_array = numpy.random.random((5,10))\n","print(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\n","print(f\"It is of type: {type(numpy_array)}\")"]},{"cell_type":"markdown","metadata":{"id":"bMYmI3QhqOWX"},"source":["You can easily cast regular numpy arrays or lists into trax numpy arrays using the `trax.fastmath.numpy.array()` function:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZX1yzwLqOWY","executionInfo":{"status":"ok","timestamp":1678169636362,"user_tz":-420,"elapsed":1680,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"60d12ab9-52bf-4373-c4f1-caee5d463da4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The trax numpy array looks like this:\n","\n"," [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n","  0.10108656 0.9284881  0.60910916 0.59655344]\n"," [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n","  0.58940125 0.04993276 0.56179184 0.7663585 ]\n"," [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n","  0.16242374 0.7093706  0.16062407 0.81077677]\n"," [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n","  0.67483455 0.7823827  0.69520164 0.32895446]\n"," [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n","  0.39807245 0.72790533 0.86333096 0.02616954]]\n","\n","It is of type: <class 'jaxlib.xla_extension.Array'>\n"]}],"source":["trax_numpy_array = np.array(numpy_array)\n","print(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\n","print(f\"It is of type: {type(trax_numpy_array)}\")"]},{"cell_type":"markdown","metadata":{"id":"SZPN6WdBqOWY"},"source":["Hope you now understand the differences (and similarities) between these two versions and numpy. **Great!**\n","\n","The previous section was a quick look at Trax's numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model.\n"]},{"cell_type":"markdown","metadata":{"id":"MrzLfZ3wqOWY"},"source":["## Calculating Perplexity"]},{"cell_type":"markdown","metadata":{"id":"FPN18kNWqOWZ"},"source":["The perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as: \n","\n","$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{i-1})}}$$\n","\n","As an implementation hack, you would usually take the log of that formula (so the computation is less prone to underflow problems). You would also need to take care of the padding, since you do not want to include the padding when calculating the perplexity (to avoid an artificially good metric).\n","\n","After taking the logarithm of $P(W)$ you have:\n","\n","$$log P(W) = {\\log\\left(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{i-1})}}\\right)}$$\n","\n","\n","$$ = \\log\\left(\\left(\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{i-1})}\\right)^{\\frac{1}{N}}\\right)$$\n","\n","$$ = \\log\\left(\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{i-1})}}\\right)^{-\\frac{1}{N}}\\right)$$\n","\n","$$ = -\\frac{1}{N}{\\log\\left({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{i-1})}}\\right)} $$\n","\n","$$ = -\\frac{1}{N}{{\\sum_{i=1}^{N}{\\log P(w_i| w_1,...,w_{i-1})}}} $$\n"]},{"cell_type":"markdown","metadata":{"id":"n4PPcGGRqOWZ"},"source":["You will be working with a real example from this week's assignment. The example is made up of:\n","   - `predictions` : log probabilities for each element in the vocabulary for 32 sequences with 64 elements (after padding).\n","   - `targets` : 32 observed sequences of 64 elements (after padding)."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkdwNUMjqOWa","executionInfo":{"status":"ok","timestamp":1678169818998,"user_tz":-420,"elapsed":1370,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"a9f3e568-7f39-46c2-8f05-5b252cd6b82b"},"outputs":[{"output_type":"stream","name":"stdout","text":["predictions has shape: (32, 64, 256)\n","targets has shape: (32, 64)\n"]}],"source":["from trax import layers as tl\n","\n","# Load from .npy files\n","predictions = numpy.load('predictions.npy')\n","targets = numpy.load('targets.npy')\n","\n","# Cast to jax.interpreters.xla.DeviceArray\n","predictions = np.array(predictions)\n","targets = np.array(targets)\n","\n","# Print shapes\n","print(f'predictions has shape: {predictions.shape}')\n","print(f'targets has shape: {targets.shape}')"]},{"cell_type":"markdown","metadata":{"id":"pPLRHdcsqOWa"},"source":["Notice that the predictions have an extra dimension with the same length as the size of the vocabulary used.\n","\n","Because of this you will need a way of reshaping `targets` to match this shape. For this you can use `trax.layers.one_hot()`.\n","\n","Notice that `predictions.shape[-1]` will return the size of the last dimension of `predictions`."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBOV1m5JqOWa","executionInfo":{"status":"ok","timestamp":1678169830024,"user_tz":-420,"elapsed":5478,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"d4b56c10-bf76-4544-a173-fed6e13c8c55"},"outputs":[{"output_type":"stream","name":"stdout","text":["reshaped_targets has shape: (32, 64, 256)\n"]}],"source":["reshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\n","print(f'reshaped_targets has shape: {reshaped_targets.shape}')"]},{"cell_type":"markdown","metadata":{"id":"wwVXQbByqOWb"},"source":["By calculating the product of the predictions and the reshaped targets and summing across the last dimension, the total log propbability of each observed element within the sequences can be computed:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zatVKo_yqOWb","executionInfo":{"status":"ok","timestamp":1678169832993,"user_tz":-420,"elapsed":592,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}}},"outputs":[],"source":["log_p = np.sum(predictions * reshaped_targets, axis= -1)"]},{"cell_type":"markdown","metadata":{"id":"YiyN5-TZqOWb"},"source":["Now you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). For identifying which elements are padding and which are not, you can use `np.equal()` and get a tensor with `1s` in the positions of actual values and `0s` where there are paddings."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIeEURVIqOWb","executionInfo":{"status":"ok","timestamp":1678169841878,"user_tz":-420,"elapsed":911,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"00a42058-3cb7-458a-ec86-e3fd79c5b661"},"outputs":[{"output_type":"stream","name":"stdout","text":["non_pad has shape: (32, 64)\n","\n","non_pad looks like this: \n","\n"," [[1. 1. 1. ... 0. 0. 0.]\n"," [1. 1. 1. ... 0. 0. 0.]\n"," [1. 1. 1. ... 0. 0. 0.]\n"," ...\n"," [1. 1. 1. ... 0. 0. 0.]\n"," [1. 1. 1. ... 0. 0. 0.]\n"," [1. 1. 1. ... 0. 0. 0.]]\n"]}],"source":["non_pad = 1.0 - np.equal(targets, 0)\n","print(f'non_pad has shape: {non_pad.shape}\\n')\n","print(f'non_pad looks like this: \\n\\n {non_pad}')"]},{"cell_type":"markdown","metadata":{"id":"zLgxQks2qOWc"},"source":["By computing the product of the log probabilities and the non_pad tensor you remove the effect of padding on the metric:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaQ9zpaCqOWc","executionInfo":{"status":"ok","timestamp":1678169843265,"user_tz":-420,"elapsed":5,"user":{"displayName":"Belajar Komputer","userId":"06731272483571470691"}},"outputId":"d3d6d218-261d-4259-ed1a-431caad4423f"},"outputs":[{"output_type":"stream","name":"stdout","text":["real log probabilities still have shape: (32, 64)\n"]}],"source":["real_log_p = log_p * non_pad\n","print(f'real log probabilities still have shape: {real_log_p.shape}')"]},{"cell_type":"markdown","metadata":{"id":"IOpUZlJoqOWc"},"source":["You can check the effect of filtering out the padding by looking at the two log probabilities tensors:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uG3Ud079qOWc"},"outputs":[],"source":["print(f'log probabilities before filtering padding: \\n\\n {log_p}\\n')\n","print(f'log probabilities after filtering padding: \\n\\n {real_log_p}')"]},{"cell_type":"markdown","metadata":{"id":"uFnxl6V1qOWc"},"source":["Finally, to get the average log perplexity of the model across all sequences in the batch, you will sum the log probabilities in each sequence and divide by the number of non padding elements (which will give you the negative log perplexity per sequence). After that, you can get the mean of the log perplexity across all sequences in the batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aCLeE4JqOWd"},"outputs":[],"source":["log_ppx = np.sum(real_log_p, axis=1) / np.sum(non_pad, axis=1)\n","log_ppx = np.mean(-log_ppx)\n","print(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')"]},{"cell_type":"markdown","metadata":{"id":"1W73DmMLqOWd"},"source":["**Congratulations on finishing this lecture notebook!** Now you should have a clear understanding of how to work with Trax's numpy and how to compute the perplexity to evaluate your language models. **Keep it up!**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}